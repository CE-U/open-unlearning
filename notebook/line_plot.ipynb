{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user_data/zhilif/anaconda3/envs/tf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/user_data/zhilif/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from scipy.stats import sem\n",
    "\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from os.path import join \n",
    "from pathlib import Path\n",
    "from scipy.stats import ks_2samp\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font_path = '../Times New Roman.ttf'\n",
    "prop = FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "# add font to font manager \n",
    "font_manager.fontManager.addfont(font_path)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5, font=prop.get_name())\n",
    "\n",
    "colors = list(sns.color_palette(\"magma\", n_colors=8))\n",
    "fs=22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_df(df):\n",
    "    mean_df = copy.deepcopy(df)\n",
    "    # delete row \"generated_text\"\n",
    "    mean_df = mean_df.drop(['generated_text', 'normalized_gt_loss'], axis=0)\n",
    "    mean_df.loc['avg_gt_prob']=np.zeros(len(mean_df.columns))\n",
    "    # for each entry in df, the entry is a dict, get the mean of the values\n",
    "    for eval_task, metrics in mean_df.items():\n",
    "        # iterate through metrics\n",
    "        for metric, res in metrics.items():\n",
    "            # get mean\n",
    "            # print(metric)\n",
    "            if metric in ['avg_gt_prob', 'forget_quality', 'truth_ratio']:\n",
    "                continue\n",
    "            mean_df[eval_task][metric] = np.mean(list(res.values()))\n",
    "\n",
    "        if 'eval_log' in eval_task:\n",
    "            perplexities = np.array(list(df[eval_task]['avg_gt_loss'].values()))\n",
    "            probs = np.exp(-1 * perplexities)\n",
    "            mean_df[eval_task]['avg_gt_prob'] = np.mean(probs)\n",
    "\n",
    "        else:\n",
    "            avg_gt_loss = df[eval_task]['avg_gt_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "            data_indices = avg_gt_loss.keys()\n",
    "            normalized_gt_prob = {}\n",
    "            for idx in data_indices:\n",
    "                truth_prob = np.exp(-1 * avg_gt_loss[idx])\n",
    "                perturb_prob = np.exp(-1 * np.array(avg_perturb_loss[idx]))\n",
    "                all_prob = np.array([truth_prob, *perturb_prob])\n",
    "                normalized_gt_prob[idx] = truth_prob / all_prob.sum()\n",
    "            mean_df[eval_task]['avg_gt_prob'] = np.mean(np.array(list(normalized_gt_prob.values())))\n",
    "        \n",
    "        if eval_task == 'eval_log_forget.json':\n",
    "            # truth_ratio = np.array(list(df[eval_task]['truth_ratio'].values()))\n",
    "            # adjusted_truth_ratio = np.minimum(truth_ratio, 1/truth_ratio)\n",
    "\n",
    "            avg_paraphrased_loss = df[eval_task]['avg_paraphrased_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "\n",
    "            data_indices = list(avg_paraphrased_loss.keys())\n",
    "            avg_paraphrased_loss = np.array([avg_paraphrased_loss[idx] for idx in data_indices])\n",
    "            avg_perturb_loss = np.array([avg_perturb_loss[idx] for idx in data_indices]).mean(-1)\n",
    "\n",
    "            truth_ratio = np.exp(avg_paraphrased_loss-avg_perturb_loss)\n",
    "            adjusted_truth_ratio = np.minimum(truth_ratio, 1/truth_ratio)\n",
    "            mean_df[eval_task]['truth_ratio'] = np.mean(adjusted_truth_ratio)\n",
    "        else:\n",
    "            avg_paraphrased_loss = df[eval_task]['avg_paraphrased_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "\n",
    "            data_indices = list(avg_paraphrased_loss.keys())\n",
    "            \n",
    "            avg_paraphrased_loss = np.array([avg_paraphrased_loss[idx] for idx in data_indices])\n",
    "            avg_perturb_loss = np.array([avg_perturb_loss[idx] for idx in data_indices]).mean(-1)\n",
    "            truth_ratio = np.exp(avg_paraphrased_loss-avg_perturb_loss)\n",
    "            \n",
    "            adjusted_truth_ratio = np.maximum(0, 1-truth_ratio)\n",
    "            mean_df[eval_task]['truth_ratio'] = np.mean(adjusted_truth_ratio)\n",
    "            \n",
    "    return mean_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forget_quality(unlearn_df, retain_df):\n",
    "    unlearn_df.loc['forget_quality']=np.zeros(len(unlearn_df.columns))\n",
    "    for eval_task, _ in unlearn_df.items():\n",
    "        if eval_task == 'eval_log_forget.json':\n",
    "            retain_truth_ratio = retain_df[eval_task]['truth_ratio']\n",
    "            unlearn_truth_ratio = unlearn_df[eval_task]['truth_ratio']\n",
    "\n",
    "            data_indices = list(retain_truth_ratio.keys())\n",
    "            retain_truth_ratio = np.array([retain_truth_ratio[idx] for idx in data_indices])\n",
    "            unlearn_truth_ratio = np.array([unlearn_truth_ratio[idx] for idx in data_indices])\n",
    "\n",
    "            ks_test = ks_2samp(retain_truth_ratio, unlearn_truth_ratio)\n",
    "            pvalue = ks_test.pvalue\n",
    "            unlearn_df[eval_task]['forget_quality'] = pvalue\n",
    "    return unlearn_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_rates=['forget01', 'forget05', 'forget10']\n",
    "model_family = ['llama2-7b', 'phi']\n",
    "retain_model_path_dict = {\n",
    "    'llama2-7b': {\n",
    "        'forget01': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain99_wd0.01/checkpoint-618',\n",
    "        'forget05': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain95_wd0.01/checkpoint-593',\n",
    "        'forget10': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain90_wd0.01/checkpoint-562'\n",
    "    },\n",
    "    'phi': {\n",
    "        'forget01': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain99_wd0.01/checkpoint-618',\n",
    "        'forget05': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain95_wd0.01/checkpoint-593',\n",
    "        'forget10': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain90_wd0.01/checkpoint-562'\n",
    "    }\n",
    "}\n",
    "retain_df_dict = {}\n",
    "for model in model_family:\n",
    "    retain_df_dict[model] = {}\n",
    "    for rate in forget_rates:\n",
    "        retain_model_path = retain_model_path_dict[model][rate]\n",
    "        retain_df_dict[model][rate] = pd.read_json(join(retain_model_path, f'eval_results/ds_size300/eval_log_aggregated.json'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family = ['llama2-7b', 'phi']\n",
    "ft_model_path = {\n",
    "    'llama2-7b': '/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_full_wd0.01/checkpoint-625',\n",
    "    'phi': '/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_full_wd0.01/checkpoint-625'\n",
    "}\n",
    "algorithms = ['grad_ascent', 'grad_diff', 'idk', 'KL']\n",
    "forget_rates=['forget01', 'forget05', 'forget10']\n",
    "lr_map = {\n",
    "    'phi': '2e-05',\n",
    "    'llama2-7b': '1e-05',\n",
    "}\n",
    "\n",
    "df_dict = {}\n",
    "for model in model_family:\n",
    "    df_dict[model] = {}\n",
    "    for algo in algorithms:\n",
    "        df_dict[model][algo] = {}\n",
    "        for rate in forget_rates:\n",
    "            ft_df = pd.read_json(join(ft_model_path[model], f'eval_results/ds_size300/eval_log_aggregated.json'))\n",
    "            retain_df = retain_df_dict[model][rate]\n",
    "            ft_df = get_forget_quality(ft_df, retain_df)\n",
    "            mean_ft_df = get_mean_df(ft_df)\n",
    "            subfolder1 = join(ft_model_path[model], f'{algo}_1e-05_{rate}')\n",
    "            # iterate through the subfolder of subfolder1 that starts with checkpoint\n",
    "            ckpt_folders = os.listdir(subfolder1)\n",
    "            ckpt_folders = natsorted([i for i in ckpt_folders if 'checkpoint' in i])\n",
    "            ckpt_df = pd.DataFrame(index=mean_ft_df.index, columns=mean_ft_df.columns)\n",
    "\n",
    "            ckpt_df_list = [mean_ft_df]\n",
    "            for ckpt in ckpt_folders:\n",
    "                eval_result_path = join(subfolder1, ckpt, 'eval_results/ds_size300/eval_log_aggregated.json')\n",
    "                eval_result = pd.read_json(eval_result_path)\n",
    "                eval_result = get_forget_quality(eval_result, retain_df)\n",
    "                mean_df = get_mean_df(eval_result)\n",
    "                ckpt_df_list.append(mean_df)\n",
    "            \n",
    "            for column in ckpt_df.columns:\n",
    "                for index in ckpt_df.index:\n",
    "                    # Concatenate the cell values across DataFrames into a list\n",
    "                    ckpt_df.at[index, column] = [df.at[index, column] for df in ckpt_df_list]\n",
    "\n",
    "            df_dict[model][algo][rate] = ckpt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_log.json</th>\n",
       "      <th>eval_real_author_wo_options.json</th>\n",
       "      <th>eval_real_world_wo_options.json</th>\n",
       "      <th>eval_log_forget.json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_gt_loss</th>\n",
       "      <td>[0.010650940249954815, 0.045219018567197775, 6...</td>\n",
       "      <td>[3.311718544960022, 2.9215847378969193, 4.7921...</td>\n",
       "      <td>[5.334272095280835, 4.742592389766987, 7.82028...</td>\n",
       "      <td>[0.00914977454134059, 0.1381961675477209, 12.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt_loss</th>\n",
       "      <td>[0.4132318326970558, 1.523618457186967, 256.88...</td>\n",
       "      <td>[18.697921714782716, 16.47484137058258, 27.211...</td>\n",
       "      <td>[22.00432986071986, 19.552523075005947, 32.564...</td>\n",
       "      <td>[0.39916553797665943, 5.559319131678591, 574.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_token_gt</th>\n",
       "      <td>[39.27333333333333, 39.27333333333333, 39.2733...</td>\n",
       "      <td>[6.03, 6.03, 6.03, 6.03, 6.03, 6.03]</td>\n",
       "      <td>[4.512820512820513, 4.512820512820513, 4.51282...</td>\n",
       "      <td>[43.44, 43.44, 43.44, 43.44, 43.44, 43.44]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge1_recall</th>\n",
       "      <td>[0.987365679302776, 0.9231376710769151, 0.1058...</td>\n",
       "      <td>[0.9329999999999999, 0.917, 0.1728333333333333...</td>\n",
       "      <td>[0.8881766381766381, 0.8945868945868946, 0.661...</td>\n",
       "      <td>[0.988206612946572, 0.791890234748079, 0.07892...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL_recall</th>\n",
       "      <td>[0.9856545937933034, 0.9094457692299227, 0.087...</td>\n",
       "      <td>[0.9329999999999999, 0.9129999999999999, 0.172...</td>\n",
       "      <td>[0.8824786324786325, 0.8888888888888888, 0.654...</td>\n",
       "      <td>[0.985449693916369, 0.757231894245776, 0.06689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_perturb_loss</th>\n",
       "      <td>[3.2774270611604055, 3.0406298926671345, 9.609...</td>\n",
       "      <td>[4.527273559172948, 4.177852221727371, 6.50179...</td>\n",
       "      <td>[6.499417964549486, 5.971800405415375, 9.49811...</td>\n",
       "      <td>[3.209800477027893, 2.9054657727877298, 13.707...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_paraphrased_loss</th>\n",
       "      <td>[2.4909248745441435, 2.277394010225932, 8.8041...</td>\n",
       "      <td>[3.310828273296356, 2.9214651638269427, 4.7908...</td>\n",
       "      <td>[5.3355035853182144, 4.743923092499758, 7.8198...</td>\n",
       "      <td>[2.476071370045344, 2.205516664584478, 13.4384...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth_ratio</th>\n",
       "      <td>[0.47469858801063763, 0.4694987259572409, 0.46...</td>\n",
       "      <td>[0.5962289157077106, 0.6164888867223018, 0.642...</td>\n",
       "      <td>[0.5390328416393139, 0.551145594485676, 0.6295...</td>\n",
       "      <td>[0.5159854212808592, 0.5307776556603724, 0.535...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paraphrased_loss</th>\n",
       "      <td>[117.34154207229614, 107.37970889409384, 402.3...</td>\n",
       "      <td>[18.691838240623476, 16.474396562576295, 27.20...</td>\n",
       "      <td>[22.007264960525383, 19.55802017603165, 32.562...</td>\n",
       "      <td>[122.2858584912618, 109.08907880783082, 657.46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perturb_loss</th>\n",
       "      <td>[147.08768336105348, 136.49786761220295, 429.2...</td>\n",
       "      <td>[25.76645954767863, 23.75718757311503, 37.6113...</td>\n",
       "      <td>[27.3067639858974, 25.05995499102818, 40.49451...</td>\n",
       "      <td>[153.53878159840903, 139.0951685028076, 662.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_token_paraphrased</th>\n",
       "      <td>[45.126666666666665, 45.126666666666665, 45.12...</td>\n",
       "      <td>[6.03, 6.03, 6.03, 6.03, 6.03, 6.03]</td>\n",
       "      <td>[4.512820512820513, 4.512820512820513, 4.51282...</td>\n",
       "      <td>[48.166666666666664, 48.166666666666664, 48.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_token_perturb</th>\n",
       "      <td>[44.46933333333333, 44.46933333333333, 44.4693...</td>\n",
       "      <td>[6.013333333333334, 6.013333333333334, 6.01333...</td>\n",
       "      <td>[4.507122507122507, 4.507122507122507, 4.50712...</td>\n",
       "      <td>[47.788, 47.788, 47.788, 47.788, 47.788, 47.788]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forget_quality</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[1.834066410994743e-21, 2.1857531667946305e-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_gt_prob</th>\n",
       "      <td>[0.9895272273969072, 0.9608036367802, 0.010722...</td>\n",
       "      <td>[0.45548207783762884, 0.4736271415687449, 0.50...</td>\n",
       "      <td>[0.41856180755948974, 0.43702570297775695, 0.4...</td>\n",
       "      <td>[0.9909385566403213, 0.8870737428383404, 0.000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           eval_log.json  \\\n",
       "avg_gt_loss            [0.010650940249954815, 0.045219018567197775, 6...   \n",
       "gt_loss                [0.4132318326970558, 1.523618457186967, 256.88...   \n",
       "num_token_gt           [39.27333333333333, 39.27333333333333, 39.2733...   \n",
       "rouge1_recall          [0.987365679302776, 0.9231376710769151, 0.1058...   \n",
       "rougeL_recall          [0.9856545937933034, 0.9094457692299227, 0.087...   \n",
       "average_perturb_loss   [3.2774270611604055, 3.0406298926671345, 9.609...   \n",
       "avg_paraphrased_loss   [2.4909248745441435, 2.277394010225932, 8.8041...   \n",
       "truth_ratio            [0.47469858801063763, 0.4694987259572409, 0.46...   \n",
       "paraphrased_loss       [117.34154207229614, 107.37970889409384, 402.3...   \n",
       "perturb_loss           [147.08768336105348, 136.49786761220295, 429.2...   \n",
       "num_token_paraphrased  [45.126666666666665, 45.126666666666665, 45.12...   \n",
       "num_token_perturb      [44.46933333333333, 44.46933333333333, 44.4693...   \n",
       "forget_quality                            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "avg_gt_prob            [0.9895272273969072, 0.9608036367802, 0.010722...   \n",
       "\n",
       "                                        eval_real_author_wo_options.json  \\\n",
       "avg_gt_loss            [3.311718544960022, 2.9215847378969193, 4.7921...   \n",
       "gt_loss                [18.697921714782716, 16.47484137058258, 27.211...   \n",
       "num_token_gt                        [6.03, 6.03, 6.03, 6.03, 6.03, 6.03]   \n",
       "rouge1_recall          [0.9329999999999999, 0.917, 0.1728333333333333...   \n",
       "rougeL_recall          [0.9329999999999999, 0.9129999999999999, 0.172...   \n",
       "average_perturb_loss   [4.527273559172948, 4.177852221727371, 6.50179...   \n",
       "avg_paraphrased_loss   [3.310828273296356, 2.9214651638269427, 4.7908...   \n",
       "truth_ratio            [0.5962289157077106, 0.6164888867223018, 0.642...   \n",
       "paraphrased_loss       [18.691838240623476, 16.474396562576295, 27.20...   \n",
       "perturb_loss           [25.76645954767863, 23.75718757311503, 37.6113...   \n",
       "num_token_paraphrased               [6.03, 6.03, 6.03, 6.03, 6.03, 6.03]   \n",
       "num_token_perturb      [6.013333333333334, 6.013333333333334, 6.01333...   \n",
       "forget_quality                            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "avg_gt_prob            [0.45548207783762884, 0.4736271415687449, 0.50...   \n",
       "\n",
       "                                         eval_real_world_wo_options.json  \\\n",
       "avg_gt_loss            [5.334272095280835, 4.742592389766987, 7.82028...   \n",
       "gt_loss                [22.00432986071986, 19.552523075005947, 32.564...   \n",
       "num_token_gt           [4.512820512820513, 4.512820512820513, 4.51282...   \n",
       "rouge1_recall          [0.8881766381766381, 0.8945868945868946, 0.661...   \n",
       "rougeL_recall          [0.8824786324786325, 0.8888888888888888, 0.654...   \n",
       "average_perturb_loss   [6.499417964549486, 5.971800405415375, 9.49811...   \n",
       "avg_paraphrased_loss   [5.3355035853182144, 4.743923092499758, 7.8198...   \n",
       "truth_ratio            [0.5390328416393139, 0.551145594485676, 0.6295...   \n",
       "paraphrased_loss       [22.007264960525383, 19.55802017603165, 32.562...   \n",
       "perturb_loss           [27.3067639858974, 25.05995499102818, 40.49451...   \n",
       "num_token_paraphrased  [4.512820512820513, 4.512820512820513, 4.51282...   \n",
       "num_token_perturb      [4.507122507122507, 4.507122507122507, 4.50712...   \n",
       "forget_quality                            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "avg_gt_prob            [0.41856180755948974, 0.43702570297775695, 0.4...   \n",
       "\n",
       "                                                    eval_log_forget.json  \n",
       "avg_gt_loss            [0.00914977454134059, 0.1381961675477209, 12.9...  \n",
       "gt_loss                [0.39916553797665943, 5.559319131678591, 574.9...  \n",
       "num_token_gt                  [43.44, 43.44, 43.44, 43.44, 43.44, 43.44]  \n",
       "rouge1_recall          [0.988206612946572, 0.791890234748079, 0.07892...  \n",
       "rougeL_recall          [0.985449693916369, 0.757231894245776, 0.06689...  \n",
       "average_perturb_loss   [3.209800477027893, 2.9054657727877298, 13.707...  \n",
       "avg_paraphrased_loss   [2.476071370045344, 2.205516664584478, 13.4384...  \n",
       "truth_ratio            [0.5159854212808592, 0.5307776556603724, 0.535...  \n",
       "paraphrased_loss       [122.2858584912618, 109.08907880783082, 657.46...  \n",
       "perturb_loss           [153.53878159840903, 139.0951685028076, 662.11...  \n",
       "num_token_paraphrased  [48.166666666666664, 48.166666666666664, 48.16...  \n",
       "num_token_perturb       [47.788, 47.788, 47.788, 47.788, 47.788, 47.788]  \n",
       "forget_quality         [1.834066410994743e-21, 2.1857531667946305e-20...  \n",
       "avg_gt_prob            [0.9909385566403213, 0.8870737428383404, 0.000...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['llama2-7b']['grad_diff']['forget10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some sanity checks\n",
    "# for model in model_family:\n",
    "#     for rate in forget_rates:\n",
    "#         df = retain_df_dict[model][rate]\n",
    "#         for eval_task, metrics in df.items():\n",
    "#             truth_ratio = np.array(list(metrics['truth_ratio'].values()))\n",
    "#             avg_paraphrased_loss = np.array(list(metrics['avg_paraphrased_loss'].values()))\n",
    "#             avg_perturb_loss = np.array(list(metrics['average_perturb_loss'].values())).mean(-1)\n",
    "#             truth_ratio2 = np.exp(avg_paraphrased_loss - avg_perturb_loss)\n",
    "#             assert np.allclose(truth_ratio, truth_ratio2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_step_map = {\n",
    "    'forget01': {\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        2: 2,\n",
    "        3: 3,\n",
    "        4: 4,\n",
    "        5: 5\n",
    "    },\n",
    "    'forget05': {\n",
    "        0: 0,\n",
    "        1: 6,\n",
    "        2: 12,\n",
    "        3: 18,\n",
    "        4: 24,\n",
    "        5: 30,\n",
    "    },\n",
    "    'forget10': {\n",
    "        0: 0,\n",
    "        1: 12,\n",
    "        2: 24,\n",
    "        3: 36,\n",
    "        4: 48,\n",
    "        5: 60,\n",
    "    }\n",
    "}\n",
    "save_alg_name = {\n",
    "    'grad_ascent': 'grad_ascent',\n",
    "    'grad_diff': 'KL', \n",
    "    'idk': 'dpo',\n",
    "    'KL': 'oracle'\n",
    "}\n",
    "for model in model_family:\n",
    "    for algo in algorithms:\n",
    "        for rate in forget_rates:\n",
    "            try:\n",
    "                ckpt_df = df_dict[model][algo][rate]\n",
    "                    \n",
    "                fig, ax = plt.subplots(1, 3, figsize=(15, 3), sharey=True)\n",
    "\n",
    "                label_names = {\n",
    "                    'rougeL_recall': 'ROUGE',\n",
    "                    'avg_gt_prob': 'Probability',\n",
    "                    'truth_ratio': 'Truth Ratio',\n",
    "                }\n",
    "                forget_rate = float(rate.split('forget')[-1])\n",
    "                retain_rate = 100 - forget_rate\n",
    "                for i, m in enumerate(['rougeL_recall', 'avg_gt_prob', 'truth_ratio']):\n",
    "                    checkpoints = [0, 1, 2, 3 ,4, 5]\n",
    "                    n_ckpts = len(checkpoints)\n",
    "\n",
    "                    if i == 0:\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_world_wo_options.json'][m][:n_ckpts], label='World Facts', color=colors[2], linewidth=2, markersize=8, marker='o')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_author_wo_options.json'][m][:n_ckpts], label='Real Authors', color=colors[0], linewidth=2, markersize=8, marker='x')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log.json'][m][:n_ckpts], label=f'Retain Set ({retain_rate}%)', color=colors[-1], linewidth=2, markersize=8, marker='*')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log_forget.json'][m][:n_ckpts], label=f'Forget Set ({int(forget_rate)}%)', color=colors[6], linewidth=2, markersize=8, linestyle='--', marker='^')\n",
    "                    else:\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_world_wo_options.json'][m][:n_ckpts], color=colors[2], linewidth=2, markersize=8, marker='o')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_real_author_wo_options.json'][m][:n_ckpts], color=colors[0], linewidth=2, markersize=8, marker='x')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log.json'][m][:n_ckpts], color=colors[-1], linewidth=2, markersize=8, marker='*')\n",
    "                        ax[i].plot(checkpoints, ckpt_df['eval_log_forget.json'][m][:n_ckpts], color=colors[6], linewidth=2, markersize=8, linestyle='--', marker='^')\n",
    "\n",
    "                    ckpt_steps = [str(ckpt_step_map[rate][i]) for i in checkpoints]\n",
    "                    ax[i].set_xticks(checkpoints, ckpt_steps)\n",
    "                    # ax[i].set_xlabel\n",
    "                    ax[i].set_xticks(ax[i].get_xticks()[::2])\n",
    "                    ax[i].set_ylim([-0.1, 1.1])\n",
    "                    if i > 0:\n",
    "                        ax[i].sharey(ax[0])\n",
    "                        # ax[i].set_yticks([])\n",
    "\n",
    "                    \n",
    "                    ax[i].set_ylabel(label_names[m], fontsize=fs)\n",
    "                    \n",
    "                    ax[i].spines['bottom'].set_color('black')\n",
    "                    ax[i].spines['left'].set_color('black')\n",
    "                    ax[i].spines['top'].set_color('black')\n",
    "                    ax[i].spines['right'].set_color('black')\n",
    "                    # ax[1].set_xlabel(f'Unlearning Steps {model, algo, rate}', fontsize=fs)\n",
    "                    ax[1].set_xlabel(f'Unlearning Steps', fontsize=fs)\n",
    "\n",
    "                    fig.legend(loc='upper center', bbox_to_anchor=(0.484, 1.15), ncol=4, fontsize=fs)\n",
    "                    # plt.title(model)\n",
    "                    # plt.xlabel('Training steps')\n",
    "                    # plt.ylabel('ROUGE')\n",
    "                    # plt.legend()\n",
    "                    # plt.show()\n",
    "                    # break\n",
    "                    fig_folder = f'./figure/all_metrics/{model}'\n",
    "                    Path(fig_folder).mkdir(parents=True, exist_ok=True)\n",
    "                    fig.savefig(f'{fig_folder}/1GPU_{save_alg_name[algo]}_1e-05_{rate}_all3metric.pdf', format='pdf', bbox_inches='tight')\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f'Error in {model} {algo} {rate} ')\n",
    "                print(e)\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
