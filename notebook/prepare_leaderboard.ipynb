{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ,index,Method,Model,WD,Forget Rate,Epoch,LR,Compute,ROUGE Real Authors,ROUGE SEM Real Authors,Truth Ratio Real Authors,Truth Ratio SEM Real Authors,Prob. Real Authors,Prob. SEM Real Authors,ROUGE-P Real Authors,ROUGE-P SEM Real Authors,TTR Real Authors,TTR SEM Real Authors,ROUGE Real World,ROUGE SEM Real World,Truth Ratio Real World,Truth Ratio SEM Real World,Prob. Real World,Prob. SEM Real World,ROUGE-P Real World,ROUGE-P SEM Real World,TTR Real World,TTR SEM Real World,ROUGE Retain,ROUGE SEM Retain,Truth Ratio Retain,Truth Ratio SEM Retain,Prob. Retain,Prob. SEM Retain,ROUGE-P Retain,ROUGE-P SEM Retain,TTR Retain,TTR SEM Retain,KS Test Retain,Wilcoxon PVal Retain,Wilcoxon Stat Retain,ROUGE Forget,ROUGE SEM Forget,Truth Ratio Forget,Truth Ratio SEM Forget,Prob. Forget,Prob. SEM Forget,ROUGE-P Forget,ROUGE-P SEM Forget,TTR Forget,TTR SEM Forget,KS Test Forget,Wilcoxon PVal Forget,Wilcoxon Stat Forget,KS Test Real Authors,KS Test PVal Real Authors,Wilcoxon PVal Real Authors,Wilcoxon Stat Real Authors,KS Test Real World,KS Test PVal Real World,Wilcoxon PVal Real World,Wilcoxon Stat Real World,KS Test PVal Retain,KS Test PVal Forget,Model Utility,Forget Quality,Submitted By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user_data/zhilif/anaconda3/envs/tf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/user_data/zhilif/anaconda3/envs/tf/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from scipy.stats import sem\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from natsort import natsorted\n",
    "import os\n",
    "from os.path import join \n",
    "from pathlib import Path\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import hmean\n",
    "font_path = '../Times New Roman.ttf'\n",
    "prop = FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = prop.get_name()\n",
    "# add font to font manager \n",
    "font_manager.fontManager.addfont(font_path)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5, font=prop.get_name())\n",
    "\n",
    "colors = list(sns.color_palette(\"magma\", n_colors=8))\n",
    "fs=22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_df(df):\n",
    "    mean_df = copy.deepcopy(df)\n",
    "    # delete row \"generated_text\"\n",
    "    mean_df = mean_df.drop(['generated_text', 'normalized_gt_loss'], axis=0)\n",
    "    mean_df.loc['avg_gt_prob']=np.zeros(len(mean_df.columns))\n",
    "    # for each entry in df, the entry is a dict, get the mean of the values\n",
    "    for eval_task, metrics in mean_df.items():\n",
    "        # iterate through metrics\n",
    "        for metric, res in metrics.items():\n",
    "            # get mean\n",
    "            # print(metric)\n",
    "            if metric in ['avg_gt_prob', 'forget_quality', 'truth_ratio']:\n",
    "                continue\n",
    "            mean_df[eval_task][metric] = np.mean(list(res.values()))\n",
    "\n",
    "        if 'eval_log' in eval_task:\n",
    "            perplexities = np.array(list(df[eval_task]['avg_gt_loss'].values()))\n",
    "            probs = np.exp(-1 * perplexities)\n",
    "            mean_df[eval_task]['avg_gt_prob'] = np.mean(probs)\n",
    "\n",
    "        else:\n",
    "            avg_gt_loss = df[eval_task]['avg_gt_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "            data_indices = avg_gt_loss.keys()\n",
    "            normalized_gt_prob = {}\n",
    "            for idx in data_indices:\n",
    "                truth_prob = np.exp(-1 * avg_gt_loss[idx])\n",
    "                perturb_prob = np.exp(-1 * np.array(avg_perturb_loss[idx]))\n",
    "                all_prob = np.array([truth_prob, *perturb_prob])\n",
    "                normalized_gt_prob[idx] = truth_prob / all_prob.sum()\n",
    "            mean_df[eval_task]['avg_gt_prob'] = np.mean(np.array(list(normalized_gt_prob.values())))\n",
    "        \n",
    "        if eval_task == 'eval_log_forget.json':\n",
    "            # truth_ratio = np.array(list(df[eval_task]['truth_ratio'].values()))\n",
    "            # adjusted_truth_ratio = np.minimum(truth_ratio, 1/truth_ratio)\n",
    "\n",
    "            avg_paraphrased_loss = df[eval_task]['avg_paraphrased_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "\n",
    "            data_indices = list(avg_paraphrased_loss.keys())\n",
    "            avg_paraphrased_loss = np.array([avg_paraphrased_loss[idx] for idx in data_indices])\n",
    "            avg_perturb_loss = np.array([avg_perturb_loss[idx] for idx in data_indices]).mean(-1)\n",
    "\n",
    "            truth_ratio = np.exp(avg_paraphrased_loss-avg_perturb_loss)\n",
    "            adjusted_truth_ratio = np.minimum(truth_ratio, 1/truth_ratio)\n",
    "            mean_df[eval_task]['truth_ratio'] = np.mean(adjusted_truth_ratio)\n",
    "        else:\n",
    "            avg_paraphrased_loss = df[eval_task]['avg_paraphrased_loss']\n",
    "            avg_perturb_loss = df[eval_task]['average_perturb_loss']\n",
    "\n",
    "            data_indices = list(avg_paraphrased_loss.keys())\n",
    "            \n",
    "            avg_paraphrased_loss = np.array([avg_paraphrased_loss[idx] for idx in data_indices])\n",
    "            avg_perturb_loss = np.array([avg_perturb_loss[idx] for idx in data_indices]).mean(-1)\n",
    "            truth_ratio = np.exp(avg_paraphrased_loss-avg_perturb_loss)\n",
    "            \n",
    "            adjusted_truth_ratio = np.maximum(0, 1-truth_ratio)\n",
    "            mean_df[eval_task]['truth_ratio'] = np.mean(adjusted_truth_ratio)\n",
    "            \n",
    "    return mean_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forget_quality(unlearn_df, retain_df):\n",
    "    unlearn_df.loc['forget_quality']=np.zeros(len(unlearn_df.columns))\n",
    "    for eval_task, _ in unlearn_df.items():\n",
    "        if eval_task == 'eval_log_forget.json':\n",
    "            retain_truth_ratio = retain_df[eval_task]['truth_ratio']\n",
    "            unlearn_truth_ratio = unlearn_df[eval_task]['truth_ratio']\n",
    "\n",
    "            # data_indices = list(retain_truth_ratio.keys())\n",
    "            # retain_truth_ratio = np.array([retain_truth_ratio[idx] for idx in data_indices])\n",
    "            # unlearn_truth_ratio = np.array([unlearn_truth_ratio[idx] for idx in data_indices])\n",
    "            # we have the assumption that the retain_df will always have a truth ratio length less than or equal to unlearn_df\n",
    "            retain_truth_ratio = np.array(list(retain_truth_ratio.values()))\n",
    "            unlearn_truth_ratio = np.array(list(unlearn_truth_ratio.values()))[-len(retain_truth_ratio):]\n",
    "\n",
    "            ks_test = ks_2samp(retain_truth_ratio, unlearn_truth_ratio)\n",
    "            pvalue = ks_test.pvalue\n",
    "            unlearn_df[eval_task]['forget_quality'] = pvalue\n",
    "    return unlearn_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_rates=['forget01', 'forget05', 'forget10']\n",
    "model_family = ['llama2-7b', 'phi']\n",
    "wd = '0.01'\n",
    "retain_model_path_dict = {\n",
    "    'llama2-7b': {\n",
    "        'forget01': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain99_wd{wd}/checkpoint-618',\n",
    "        'forget05': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain95_wd{wd}/checkpoint-593',\n",
    "        'forget10': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_retain90_wd{wd}/checkpoint-562'\n",
    "    },\n",
    "    'phi': {\n",
    "        'forget01': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain99_wd{wd}/checkpoint-618',\n",
    "        'forget05': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain95_wd{wd}/checkpoint-593',\n",
    "        'forget10': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_retain90_wd{wd}/checkpoint-562'\n",
    "    }\n",
    "}\n",
    "retain_df_dict = {}\n",
    "mean_retain_df_dict = {}\n",
    "for model in model_family:\n",
    "    retain_df_dict[model] = {}\n",
    "    mean_retain_df_dict[model] = {}\n",
    "    for rate in forget_rates:\n",
    "        retain_model_path = retain_model_path_dict[model][rate]\n",
    "        retain_df_dict[model][rate] = pd.read_json(join(retain_model_path, f'eval_results/ds_size300/eval_log_aggregated.json'))\n",
    "        mean_retain_df_dict[model][rate] = get_mean_df(retain_df_dict[model][rate])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_family = ['llama2-7b', 'phi']\n",
    "ft_model_path = {\n",
    "    'llama2-7b': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr1e-05_llama2-7b_full_wd{wd}/checkpoint-625',\n",
    "    'phi': f'/home/zhilif/tofu/paper_models/ft_epoch5_lr2e-05_phi_full_wd{wd}/checkpoint-625'\n",
    "}\n",
    "algorithms = ['grad_ascent', 'grad_diff', 'idk', 'KL']\n",
    "forget_rates=['forget01', 'forget05', 'forget10']\n",
    "lr_map = {\n",
    "    'phi': '2e-05',\n",
    "    'llama2-7b': '1e-05',\n",
    "}\n",
    "\n",
    "df_dict = {}\n",
    "for model in model_family:\n",
    "    df_dict[model] = {}\n",
    "    for algo in algorithms:\n",
    "        df_dict[model][algo] = {}\n",
    "        for rate in forget_rates:\n",
    "            ft_df = pd.read_json(join(ft_model_path[model], f'eval_results/ds_size300/eval_log_aggregated.json'))\n",
    "            retain_df = retain_df_dict[model][rate]\n",
    "            ft_df = get_forget_quality(ft_df, retain_df)\n",
    "            mean_ft_df = get_mean_df(ft_df)\n",
    "            subfolder1 = join(ft_model_path[model], f'{algo}_1e-05_{rate}')\n",
    "            # iterate through the subfolder of subfolder1 that starts with checkpoint\n",
    "            ckpt_folders = os.listdir(subfolder1)\n",
    "            ckpt_folders = natsorted([i for i in ckpt_folders if 'checkpoint' in i])\n",
    "            ckpt_df = pd.DataFrame(index=mean_ft_df.index, columns=mean_ft_df.columns)\n",
    "\n",
    "            ckpt_df_list = [mean_ft_df]\n",
    "            for ckpt in ckpt_folders:\n",
    "                eval_result_path = join(subfolder1, ckpt, 'eval_results/ds_size300/eval_log_aggregated.json')\n",
    "                eval_result = pd.read_json(eval_result_path)\n",
    "                eval_result = get_forget_quality(eval_result, retain_df)\n",
    "                mean_df = get_mean_df(eval_result)\n",
    "                ckpt_df_list.append(mean_df)\n",
    "            \n",
    "            for column in ckpt_df.columns:\n",
    "                for index in ckpt_df.index:\n",
    "                    # Concatenate the cell values across DataFrames into a list\n",
    "                    ckpt_df.at[index, column] = [df.at[index, column] for df in ckpt_df_list]\n",
    "\n",
    "            df_dict[model][algo][rate] = ckpt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pareto_df(ckpt_df, retain_df):\n",
    "    # ckpt_df has to be a dataframe where the cells are lists of checkpoint metrics, it should contain both fineuned and unlearned models\n",
    "    model_utility_metrics = ['avg_gt_prob', 'rougeL_recall', 'truth_ratio']\n",
    "    # create a new df to store the pareto front\n",
    "    # make sure pareto_df uses double precision\n",
    "    utility_metric_dict = {\n",
    "        'avg_gt_prob': 'Prob.',\n",
    "        'rougeL_recall': 'ROUGE',\n",
    "        'truth_ratio': 'Truth Ratio'\n",
    "    }\n",
    "    forget_task = 'eval_log_forget.json'\n",
    "    utility_task = ['eval_log.json', 'eval_real_author_wo_options.json', 'eval_real_world_wo_options.json']\n",
    "\n",
    "    dataset_name_dict = {\n",
    "        'eval_log.json': 'Retain',\n",
    "        'eval_real_author_wo_options.json': 'Real Authors',\n",
    "        'eval_real_world_wo_options.json': 'Real World',\n",
    "        'eval_log_forget.json': 'Forget'\n",
    "    }\n",
    "\n",
    "    df_rows = ['Forget Quality', 'Model Utility']\n",
    "    # take all combination between dataset_name and model_utility_metrics\n",
    "    for dataset_name in dataset_name_dict.values():\n",
    "        for metric in utility_metric_dict.values():\n",
    "            df_rows.append(f'{metric} {dataset_name}')\n",
    "\n",
    "\n",
    "    pareto_df = pd.DataFrame(\n",
    "        index=df_rows, \n",
    "        columns=['Finetune Model', 'Retain Model', 'Unlearn Model']\n",
    "    )\n",
    "\n",
    "\n",
    "    ckpt_forget_quality = np.array(ckpt_df[forget_task]['forget_quality'], dtype=np.float64)\n",
    "    ft_forget_quality = np.array(ckpt_forget_quality[0], dtype=np.float64)\n",
    "    unlearn_forget_quality = np.array(ckpt_forget_quality[1:], dtype=np.float64)\n",
    "    retain_forget_quality = 1 # this is by definition \n",
    "    # allow float 64 \n",
    "    pareto_df.at['Forget Quality', 'Finetune Model'] = ft_forget_quality\n",
    "    pareto_df.at['Forget Quality', 'Retain Model'] = retain_forget_quality\n",
    "    pareto_df.at['Forget Quality', 'Unlearn Model'] = unlearn_forget_quality\n",
    "\n",
    "    unlearn_utilities = []\n",
    "    ft_utilities = []\n",
    "    retain_utilities = []\n",
    "\n",
    "    unlearn_forgets = []\n",
    "    ft_forgets = []\n",
    "    retain_forgets = []\n",
    "\n",
    "    for task in utility_task:\n",
    "        for metric in model_utility_metrics:\n",
    "            ckpt_metric = ckpt_df[task][metric]\n",
    "            ft_metric = ckpt_metric[0]\n",
    "            unlearn_metric = ckpt_metric[1:]\n",
    "            retain_metric = retain_df[task][metric]\n",
    "            \n",
    "            ft_utilities.append(ft_metric)\n",
    "            unlearn_utilities.append(unlearn_metric)\n",
    "            retain_utilities.append(retain_metric)\n",
    "\n",
    "            pareto_df.at[f'{utility_metric_dict[metric]} {dataset_name_dict[task]}', 'Finetune Model'] = ft_metric\n",
    "            pareto_df.at[f'{utility_metric_dict[metric]} {dataset_name_dict[task]}', 'Retain Model'] = retain_metric\n",
    "            pareto_df.at[f'{utility_metric_dict[metric]} {dataset_name_dict[task]}', 'Unlearn Model'] = unlearn_metric\n",
    "    \n",
    "    for metric in model_utility_metrics:\n",
    "        ckpt_metric = ckpt_df[forget_task][metric]\n",
    "        ft_metric = ckpt_metric[0]\n",
    "        unlearn_metric = ckpt_metric[1:]\n",
    "        retain_metric = retain_df[forget_task][metric]\n",
    "\n",
    "        pareto_df.at[f'{utility_metric_dict[metric]} {dataset_name_dict[forget_task]}', 'Finetune Model'] = ft_metric\n",
    "        pareto_df.at[f'{utility_metric_dict[metric]} {dataset_name_dict[forget_task]}', 'Retain Model'] = retain_metric\n",
    "        pareto_df.at[f'{utility_metric_dict[metric]} {dataset_name_dict[forget_task]}', 'Unlearn Model'] = unlearn_metric\n",
    "\n",
    "    # get harmonic mean\n",
    "    ft_utilities_hmean = hmean(np.array(ft_utilities, dtype=np.float64))\n",
    "    unlearn_utilities_hmean = hmean(np.array(unlearn_utilities, dtype=np.float64), axis=0)\n",
    "    retain_utilities_hmean = hmean(np.array(retain_utilities, dtype=np.float64))\n",
    "    pareto_df.at['Model Utility', 'Finetune Model'] = ft_utilities_hmean\n",
    "    pareto_df.at['Model Utility', 'Retain Model'] = retain_utilities_hmean\n",
    "    pareto_df.at['Model Utility', 'Unlearn Model'] = unlearn_utilities_hmean\n",
    "\n",
    "    return pareto_df\n",
    "\n",
    "pareto_df_dict = {}\n",
    "for model in model_family:\n",
    "    pareto_df_dict[model] = {}\n",
    "    for algo in algorithms:\n",
    "        pareto_df_dict[model][algo] = {}\n",
    "        for rate in forget_rates:\n",
    "            pareto_df_dict[model][algo][rate] = get_pareto_df(df_dict[model][algo][rate], mean_retain_df_dict[model][rate])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pareto_df_dict['llama2-7b']['grad_diff']['forget10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62820033, 0.07407765, 0.51850603, 0.58226984, 0.58721712])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.at['Model Utility', 'Unlearn Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Forget Quality', 'Model Utility', 'Prob. Retain', 'ROUGE Retain',\n",
       "       'Truth Ratio Retain', 'Prob. Real Authors', 'ROUGE Real Authors',\n",
       "       'Truth Ratio Real Authors', 'Prob. Real World', 'ROUGE Real World',\n",
       "       'Truth Ratio Real World', 'Prob. Forget', 'ROUGE Forget',\n",
       "       'Truth Ratio Forget'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finetune Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0012708143485281624</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.990939</td>\n",
       "      <td>0.98545</td>\n",
       "      <td>0.515985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retain Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.620744</td>\n",
       "      <td>0.990596</td>\n",
       "      <td>0.988945</td>\n",
       "      <td>0.464641</td>\n",
       "      <td>0.459313</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.603727</td>\n",
       "      <td>0.41217</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.538977</td>\n",
       "      <td>0.179521</td>\n",
       "      <td>0.397237</td>\n",
       "      <td>0.684941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Epoch         Forget Quality Model Utility Prob. Retain  \\\n",
       "0  Finetune Model     -1  0.0012708143485281624      0.622677     0.989527   \n",
       "1    Retain Model     -1                      1      0.620744     0.990596   \n",
       "\n",
       "  ROUGE Retain Truth Ratio Retain Prob. Real Authors ROUGE Real Authors  \\\n",
       "0     0.985655           0.474699           0.455482              0.933   \n",
       "1     0.988945           0.464641           0.459313              0.928   \n",
       "\n",
       "  Truth Ratio Real Authors Prob. Real World ROUGE Real World  \\\n",
       "0                 0.596229         0.418562         0.882479   \n",
       "1                 0.603727          0.41217         0.882479   \n",
       "\n",
       "  Truth Ratio Real World Prob. Forget ROUGE Forget Truth Ratio Forget  \n",
       "0               0.539033     0.990939      0.98545           0.515985  \n",
       "1               0.538977     0.179521     0.397237           0.684941  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a new column says \"epoch\", and unfold the row of \"unlearn model\"\n",
    "new_df = temp.T\n",
    "\n",
    "# delete the row of \"unlearn model\"\n",
    "new_df = new_df.drop('Unlearn Model')\n",
    "\n",
    "# add a new column says \"epoch\", and unfold the row of \"unlearn model\"\n",
    "new_df.insert(0, 'Epoch', [-1 for _ in range(2)])\n",
    "new_df = new_df.reset_index()\n",
    "# rename the index column to \"Model\"\n",
    "new_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unlearning Method</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>LR</th>\n",
       "      <th>WD</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finetune Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0012708143485281624</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.990939</td>\n",
       "      <td>0.98545</td>\n",
       "      <td>0.515985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retain Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.620744</td>\n",
       "      <td>0.990596</td>\n",
       "      <td>0.988945</td>\n",
       "      <td>0.464641</td>\n",
       "      <td>0.459313</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.603727</td>\n",
       "      <td>0.41217</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.538977</td>\n",
       "      <td>0.179521</td>\n",
       "      <td>0.397237</td>\n",
       "      <td>0.684941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlearn Model</td>\n",
       "      <td>1</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.992639</td>\n",
       "      <td>0.94326</td>\n",
       "      <td>0.537666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unlearn Model</td>\n",
       "      <td>2</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.6206</td>\n",
       "      <td>0.988565</td>\n",
       "      <td>0.981648</td>\n",
       "      <td>0.475066</td>\n",
       "      <td>0.452178</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.592565</td>\n",
       "      <td>0.417651</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.538557</td>\n",
       "      <td>0.910844</td>\n",
       "      <td>0.795152</td>\n",
       "      <td>0.537782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unlearn Model</td>\n",
       "      <td>3</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.618734</td>\n",
       "      <td>0.988213</td>\n",
       "      <td>0.979334</td>\n",
       "      <td>0.475905</td>\n",
       "      <td>0.448936</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.590214</td>\n",
       "      <td>0.416042</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.53751</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.685197</td>\n",
       "      <td>0.540144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unlearn Model</td>\n",
       "      <td>4</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.609718</td>\n",
       "      <td>0.976377</td>\n",
       "      <td>0.925874</td>\n",
       "      <td>0.476145</td>\n",
       "      <td>0.437524</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.57789</td>\n",
       "      <td>0.410665</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.537241</td>\n",
       "      <td>0.563045</td>\n",
       "      <td>0.552223</td>\n",
       "      <td>0.556348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Unlearn Model</td>\n",
       "      <td>5</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.606098</td>\n",
       "      <td>0.96861</td>\n",
       "      <td>0.899046</td>\n",
       "      <td>0.475981</td>\n",
       "      <td>0.433108</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.571714</td>\n",
       "      <td>0.408674</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.538149</td>\n",
       "      <td>0.45992</td>\n",
       "      <td>0.490859</td>\n",
       "      <td>0.562339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Unlearn Model</td>\n",
       "      <td>6</td>\n",
       "      <td>2e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.028603</td>\n",
       "      <td>0.602467</td>\n",
       "      <td>0.962288</td>\n",
       "      <td>0.882109</td>\n",
       "      <td>0.475767</td>\n",
       "      <td>0.428013</td>\n",
       "      <td>0.9005</td>\n",
       "      <td>0.566283</td>\n",
       "      <td>0.406492</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.539054</td>\n",
       "      <td>0.392044</td>\n",
       "      <td>0.481839</td>\n",
       "      <td>0.569459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unlearning Method  Epoch     LR    WD         Forget Quality Model Utility  \\\n",
       "0    Finetune Model     -1  2e-05  0.01  0.0012708143485281624      0.622677   \n",
       "1      Retain Model     -1  2e-05  0.01                      1      0.620744   \n",
       "2     Unlearn Model      1  2e-05  0.01               0.003018      0.622677   \n",
       "3     Unlearn Model      2  2e-05  0.01               0.001271        0.6206   \n",
       "4     Unlearn Model      3  2e-05  0.01               0.001271      0.618734   \n",
       "5     Unlearn Model      4  2e-05  0.01               0.006761      0.609718   \n",
       "6     Unlearn Model      5  2e-05  0.01               0.014302      0.606098   \n",
       "7     Unlearn Model      6  2e-05  0.01               0.028603      0.602467   \n",
       "\n",
       "  Prob. Retain ROUGE Retain Truth Ratio Retain Prob. Real Authors  \\\n",
       "0     0.989527     0.985655           0.474699           0.455482   \n",
       "1     0.990596     0.988945           0.464641           0.459313   \n",
       "2     0.989527     0.985655           0.474699           0.455482   \n",
       "3     0.988565     0.981648           0.475066           0.452178   \n",
       "4     0.988213     0.979334           0.475905           0.448936   \n",
       "5     0.976377     0.925874           0.476145           0.437524   \n",
       "6      0.96861     0.899046           0.475981           0.433108   \n",
       "7     0.962288     0.882109           0.475767           0.428013   \n",
       "\n",
       "  ROUGE Real Authors Truth Ratio Real Authors Prob. Real World  \\\n",
       "0              0.933                 0.596229         0.418562   \n",
       "1              0.928                 0.603727          0.41217   \n",
       "2              0.933                 0.596229         0.418562   \n",
       "3              0.923                 0.592565         0.417651   \n",
       "4              0.913                 0.590214         0.416042   \n",
       "5              0.898                  0.57789         0.410665   \n",
       "6             0.9055                 0.571714         0.408674   \n",
       "7             0.9005                 0.566283         0.406492   \n",
       "\n",
       "  ROUGE Real World Truth Ratio Real World Prob. Forget ROUGE Forget  \\\n",
       "0         0.882479               0.539033     0.990939      0.98545   \n",
       "1         0.882479               0.538977     0.179521     0.397237   \n",
       "2         0.882479               0.539033     0.992639      0.94326   \n",
       "3         0.882479               0.538557     0.910844     0.795152   \n",
       "4         0.884615                0.53751     0.812619     0.685197   \n",
       "5         0.884615               0.537241     0.563045     0.552223   \n",
       "6         0.880342               0.538149      0.45992     0.490859   \n",
       "7         0.880342               0.539054     0.392044     0.481839   \n",
       "\n",
       "  Truth Ratio Forget  \n",
       "0           0.515985  \n",
       "1           0.684941  \n",
       "2           0.537666  \n",
       "3           0.537782  \n",
       "4           0.540144  \n",
       "5           0.556348  \n",
       "6           0.562339  \n",
       "7           0.569459  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a new column says \"epoch\", and unfold the row of \"unlearn model\"\n",
    "new_df = temp.T\n",
    "# delete the row of \"unlearn model\"\n",
    "new_df = new_df.drop('Unlearn Model')\n",
    "# add a new column says \"epoch\", and unfold the row of \"unlearn model\"\n",
    "new_df.insert(0, 'Epoch', [-1 for _ in range(2)])\n",
    "new_df = new_df.reset_index()\n",
    "# rename the index column to \"Model\"\n",
    "new_df.rename(columns={'index': 'Unlearning Method'}, inplace=True)\n",
    "# get the row name of temp \n",
    "row_names = temp.index\n",
    "total_epochs = 6\n",
    "for i in range(total_epochs):\n",
    "    unlearn_stat = {'Unlearning Method': 'Unlearn Model',\n",
    "                    'Epoch': i+1}\n",
    "    \n",
    "    for r in row_names:\n",
    "        # print(r)\n",
    "        curr = temp.at[r, 'Unlearn Model'][i]\n",
    "        unlearn_stat[r] = curr\n",
    "    \n",
    "    \n",
    "    new_df = pd.concat([new_df, pd.DataFrame([unlearn_stat])], ignore_index=True)\n",
    "\n",
    "\n",
    "# add a new column \"WD\" \n",
    "new_df.insert(2, 'WD', wd)\n",
    "new_df.insert(2, 'LR', lr_map[model])\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create csv files for the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['grad_ascent', 'grad_diff', 'idk', 'KL']\n",
    "algorithm_names = {\n",
    "    'grad_ascent': 'Grad. Ascent',\n",
    "    'grad_diff': 'Grad. Diff.',\n",
    "    'idk': 'Pref. Opt.',\n",
    "    'KL': 'KL Min.'\n",
    "}\n",
    "model_family = ['llama2-7b', 'phi']\n",
    "model_names = {\n",
    "    'llama2-7b': 'LLaMA 2-7B',\n",
    "    'phi': 'Phi'\n",
    "}\n",
    "forget_rates=['forget01', 'forget05', 'forget10']\n",
    "lr_map = {\n",
    "    'phi': '2e-05',\n",
    "    'llama2-7b': '1e-05',\n",
    "}\n",
    "csv_dict = {}\n",
    "\n",
    "for model in model_family:\n",
    "    csv_dict[model] = {}\n",
    "    for rate in forget_rates:\n",
    "        csv_dict[model][rate] = {}\n",
    "        temp = pareto_df_dict[model]['grad_ascent'][rate]\n",
    "        new_df = temp.T\n",
    "        # delete the row of \"unlearn model\"\n",
    "        new_df = new_df.drop('Unlearn Model')\n",
    "        # add a new column says \"epoch\", and unfold the row of \"unlearn model\"\n",
    "        new_df.insert(0, 'Epoch', [-1 for _ in range(2)])\n",
    "        new_df = new_df.reset_index()\n",
    "        # rename the index column to \"Model\"\n",
    "        new_df.rename(columns={'index': 'Method'}, inplace=True)\n",
    "        # get the row name of temp \n",
    "        row_names = temp.index\n",
    "        for algo in algorithms:\n",
    "            res_df = pareto_df_dict[model][algo][rate]\n",
    "            # create a new df with the rows of res_df as its column \n",
    "\n",
    "            total_epochs = 5\n",
    "            for i in range(total_epochs):\n",
    "                unlearn_stat = {'Method': algorithm_names[algo],\n",
    "                                'Epoch': i+1}\n",
    "                \n",
    "                for r in row_names:\n",
    "                    # print(r)\n",
    "                    curr = res_df.at[r, 'Unlearn Model'][i]\n",
    "                    unlearn_stat[r] = curr\n",
    "                \n",
    "                \n",
    "                new_df = pd.concat([new_df, pd.DataFrame([unlearn_stat])], ignore_index=True)\n",
    "        # add a new column \"WD\" \n",
    "        new_df.insert(2, 'WD', wd)\n",
    "        new_df.insert(2, 'LR', lr_map[model])\n",
    "        new_df.insert(2, 'Model', model_names[model])\n",
    "        new_df.insert(2, 'Submitted By', 'Baseline')\n",
    "        # add dummy index\n",
    "        new_df.insert(0, 'Index', [i for i in range(len(new_df))])\n",
    "        csv_dict[model][rate] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Method</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Submitted By</th>\n",
       "      <th>Model</th>\n",
       "      <th>LR</th>\n",
       "      <th>WD</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Finetune Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.834066410994743e-21</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.990939</td>\n",
       "      <td>0.98545</td>\n",
       "      <td>0.515985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Retain Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613745</td>\n",
       "      <td>0.988897</td>\n",
       "      <td>0.975811</td>\n",
       "      <td>0.470977</td>\n",
       "      <td>0.434087</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.570681</td>\n",
       "      <td>0.414326</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.544198</td>\n",
       "      <td>0.147563</td>\n",
       "      <td>0.408244</td>\n",
       "      <td>0.674019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626927</td>\n",
       "      <td>0.82105</td>\n",
       "      <td>0.696747</td>\n",
       "      <td>0.429556</td>\n",
       "      <td>0.505168</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.659134</td>\n",
       "      <td>0.478685</td>\n",
       "      <td>0.921652</td>\n",
       "      <td>0.608509</td>\n",
       "      <td>0.738935</td>\n",
       "      <td>0.590409</td>\n",
       "      <td>0.59492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.045413</td>\n",
       "      <td>0.248366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155356</td>\n",
       "      <td>0.260379</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>0.161269</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.912406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044498</td>\n",
       "      <td>0.241301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382485</td>\n",
       "      <td>0.260301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.88564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046958</td>\n",
       "      <td>0.239098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.379761</td>\n",
       "      <td>0.246799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.379917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.89291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076329</td>\n",
       "      <td>0.232094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368037</td>\n",
       "      <td>0.239321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.382997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.847072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6282</td>\n",
       "      <td>0.960804</td>\n",
       "      <td>0.909446</td>\n",
       "      <td>0.469499</td>\n",
       "      <td>0.473627</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.616489</td>\n",
       "      <td>0.437026</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.551146</td>\n",
       "      <td>0.887074</td>\n",
       "      <td>0.757232</td>\n",
       "      <td>0.530778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.074078</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.087016</td>\n",
       "      <td>0.46335</td>\n",
       "      <td>0.505903</td>\n",
       "      <td>0.172833</td>\n",
       "      <td>0.642398</td>\n",
       "      <td>0.465562</td>\n",
       "      <td>0.654843</td>\n",
       "      <td>0.629567</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.066892</td>\n",
       "      <td>0.535542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.518506</td>\n",
       "      <td>0.326741</td>\n",
       "      <td>0.321388</td>\n",
       "      <td>0.505291</td>\n",
       "      <td>0.606337</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.763622</td>\n",
       "      <td>0.49307</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.650743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.789577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.58227</td>\n",
       "      <td>0.490357</td>\n",
       "      <td>0.445043</td>\n",
       "      <td>0.495036</td>\n",
       "      <td>0.583738</td>\n",
       "      <td>0.801333</td>\n",
       "      <td>0.74513</td>\n",
       "      <td>0.472397</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.622871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.816153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587217</td>\n",
       "      <td>0.54691</td>\n",
       "      <td>0.464965</td>\n",
       "      <td>0.492086</td>\n",
       "      <td>0.563388</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.729869</td>\n",
       "      <td>0.463678</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.606132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.822008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255889</td>\n",
       "      <td>0.799247</td>\n",
       "      <td>0.060264</td>\n",
       "      <td>0.412807</td>\n",
       "      <td>0.439537</td>\n",
       "      <td>0.188667</td>\n",
       "      <td>0.568402</td>\n",
       "      <td>0.433564</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.527818</td>\n",
       "      <td>0.769494</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>0.604694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351272</td>\n",
       "      <td>0.880414</td>\n",
       "      <td>0.521157</td>\n",
       "      <td>0.426914</td>\n",
       "      <td>0.402868</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.505903</td>\n",
       "      <td>0.391226</td>\n",
       "      <td>0.663818</td>\n",
       "      <td>0.479072</td>\n",
       "      <td>0.827683</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>0.580061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526714</td>\n",
       "      <td>0.918007</td>\n",
       "      <td>0.61773</td>\n",
       "      <td>0.451284</td>\n",
       "      <td>0.399881</td>\n",
       "      <td>0.561333</td>\n",
       "      <td>0.498018</td>\n",
       "      <td>0.388501</td>\n",
       "      <td>0.830484</td>\n",
       "      <td>0.47739</td>\n",
       "      <td>0.850011</td>\n",
       "      <td>0.122411</td>\n",
       "      <td>0.555596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538948</td>\n",
       "      <td>0.934998</td>\n",
       "      <td>0.745274</td>\n",
       "      <td>0.454879</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.544667</td>\n",
       "      <td>0.506812</td>\n",
       "      <td>0.392482</td>\n",
       "      <td>0.830484</td>\n",
       "      <td>0.485031</td>\n",
       "      <td>0.849341</td>\n",
       "      <td>0.091742</td>\n",
       "      <td>0.547643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535652</td>\n",
       "      <td>0.941852</td>\n",
       "      <td>0.771239</td>\n",
       "      <td>0.455014</td>\n",
       "      <td>0.403923</td>\n",
       "      <td>0.504667</td>\n",
       "      <td>0.505834</td>\n",
       "      <td>0.390857</td>\n",
       "      <td>0.847578</td>\n",
       "      <td>0.482302</td>\n",
       "      <td>0.848011</td>\n",
       "      <td>0.056527</td>\n",
       "      <td>0.546994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>KL Min.</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630474</td>\n",
       "      <td>0.838561</td>\n",
       "      <td>0.723246</td>\n",
       "      <td>0.434337</td>\n",
       "      <td>0.504439</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.658325</td>\n",
       "      <td>0.47812</td>\n",
       "      <td>0.910969</td>\n",
       "      <td>0.60741</td>\n",
       "      <td>0.754145</td>\n",
       "      <td>0.612712</td>\n",
       "      <td>0.589625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>KL Min.</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.039675</td>\n",
       "      <td>0.284137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275041</td>\n",
       "      <td>0.345207</td>\n",
       "      <td>0.052707</td>\n",
       "      <td>0.363304</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.932414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>KL Min.</td>\n",
       "      <td>3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075642</td>\n",
       "      <td>0.272774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.433005</td>\n",
       "      <td>0.356455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>KL Min.</td>\n",
       "      <td>4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066558</td>\n",
       "      <td>0.277158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.385758</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>KL Min.</td>\n",
       "      <td>5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>LLaMA 2-7B</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064234</td>\n",
       "      <td>0.272854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.385826</td>\n",
       "      <td>0.248229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.864236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index          Method  Epoch Submitted By       Model     LR    WD  \\\n",
       "0       0  Finetune Model     -1     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "1       1    Retain Model     -1     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "2       2    Grad. Ascent      1     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "3       3    Grad. Ascent      2     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "4       4    Grad. Ascent      3     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "5       5    Grad. Ascent      4     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "6       6    Grad. Ascent      5     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "7       7     Grad. Diff.      1     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "8       8     Grad. Diff.      2     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "9       9     Grad. Diff.      3     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "10     10     Grad. Diff.      4     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "11     11     Grad. Diff.      5     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "12     12      Pref. Opt.      1     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "13     13      Pref. Opt.      2     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "14     14      Pref. Opt.      3     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "15     15      Pref. Opt.      4     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "16     16      Pref. Opt.      5     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "17     17         KL Min.      1     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "18     18         KL Min.      2     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "19     19         KL Min.      3     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "20     20         KL Min.      4     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "21     21         KL Min.      5     Baseline  LLaMA 2-7B  1e-05  0.01   \n",
       "\n",
       "           Forget Quality Model Utility Prob. Retain ROUGE Retain  \\\n",
       "0   1.834066410994743e-21      0.622677     0.989527     0.985655   \n",
       "1                       1      0.613745     0.988897     0.975811   \n",
       "2                     0.0      0.626927      0.82105     0.696747   \n",
       "3                     0.0           0.0     0.000068     0.000123   \n",
       "4                     0.0           0.0          0.0          0.0   \n",
       "5                     0.0           0.0          0.0          0.0   \n",
       "6                     0.0           0.0          0.0          0.0   \n",
       "7                     0.0        0.6282     0.960804     0.909446   \n",
       "8                0.000334      0.074078     0.010723     0.087016   \n",
       "9                     0.0      0.518506     0.326741     0.321388   \n",
       "10                    0.0       0.58227     0.490357     0.445043   \n",
       "11                    0.0      0.587217      0.54691     0.464965   \n",
       "12                    0.0      0.255889     0.799247     0.060264   \n",
       "13                    0.0      0.351272     0.880414     0.521157   \n",
       "14                    0.0      0.526714     0.918007      0.61773   \n",
       "15                    0.0      0.538948     0.934998     0.745274   \n",
       "16                    0.0      0.535652     0.941852     0.771239   \n",
       "17                    0.0      0.630474     0.838561     0.723246   \n",
       "18                    0.0           0.0     0.000078     0.003866   \n",
       "19                    0.0           0.0          0.0          0.0   \n",
       "20                    0.0           0.0          0.0          0.0   \n",
       "21                    0.0           0.0          0.0          0.0   \n",
       "\n",
       "   Truth Ratio Retain Prob. Real Authors ROUGE Real Authors  \\\n",
       "0            0.474699           0.455482              0.933   \n",
       "1            0.470977           0.434087              0.923   \n",
       "2            0.429556           0.505168              0.943   \n",
       "3            0.045413           0.248366                0.0   \n",
       "4            0.044498           0.241301                0.0   \n",
       "5            0.046958           0.239098                0.0   \n",
       "6            0.076329           0.232094                0.0   \n",
       "7            0.469499           0.473627              0.913   \n",
       "8             0.46335           0.505903           0.172833   \n",
       "9            0.505291           0.606337              0.686   \n",
       "10           0.495036           0.583738           0.801333   \n",
       "11           0.492086           0.563388           0.807333   \n",
       "12           0.412807           0.439537           0.188667   \n",
       "13           0.426914           0.402868              0.104   \n",
       "14           0.451284           0.399881           0.561333   \n",
       "15           0.454879           0.405405           0.544667   \n",
       "16           0.455014           0.403923           0.504667   \n",
       "17           0.434337           0.504439              0.943   \n",
       "18           0.039675           0.284137                0.0   \n",
       "19           0.075642           0.272774                0.0   \n",
       "20           0.066558           0.277158                0.0   \n",
       "21           0.064234           0.272854                0.0   \n",
       "\n",
       "   Truth Ratio Real Authors Prob. Real World ROUGE Real World  \\\n",
       "0                  0.596229         0.418562         0.882479   \n",
       "1                  0.570681         0.414326         0.897436   \n",
       "2                  0.659134         0.478685         0.921652   \n",
       "3                  0.155356         0.260379         0.007123   \n",
       "4                  0.382485         0.260301              0.0   \n",
       "5                  0.379761         0.246799              0.0   \n",
       "6                  0.368037         0.239321              0.0   \n",
       "7                  0.616489         0.437026         0.888889   \n",
       "8                  0.642398         0.465562         0.654843   \n",
       "9                  0.763622          0.49307         0.816239   \n",
       "10                  0.74513         0.472397         0.888889   \n",
       "11                 0.729869         0.463678         0.888889   \n",
       "12                 0.568402         0.433564         0.730769   \n",
       "13                 0.505903         0.391226         0.663818   \n",
       "14                 0.498018         0.388501         0.830484   \n",
       "15                 0.506812         0.392482         0.830484   \n",
       "16                 0.505834         0.390857         0.847578   \n",
       "17                 0.658325          0.47812         0.910969   \n",
       "18                 0.275041         0.345207         0.052707   \n",
       "19                 0.433005         0.356455              0.0   \n",
       "20                 0.385758         0.259533              0.0   \n",
       "21                 0.385826         0.248229              0.0   \n",
       "\n",
       "   Truth Ratio Real World Prob. Forget ROUGE Forget Truth Ratio Forget  \n",
       "0                0.539033     0.990939      0.98545           0.515985  \n",
       "1                0.544198     0.147563     0.408244           0.674019  \n",
       "2                0.608509     0.738935     0.590409            0.59492  \n",
       "3                0.161269     0.000068     0.000403           0.912406  \n",
       "4                0.391465          0.0     0.002029            0.88564  \n",
       "5                0.379917          0.0     0.001943            0.89291  \n",
       "6                0.382997          0.0     0.001744           0.847072  \n",
       "7                0.551146     0.887074     0.757232           0.530778  \n",
       "8                0.629567     0.000376     0.066892           0.535542  \n",
       "9                0.650743          0.0     0.003431           0.789577  \n",
       "10               0.622871          0.0     0.003431           0.816153  \n",
       "11               0.606132          0.0     0.002444           0.822008  \n",
       "12               0.527818     0.769494     0.009444           0.604694  \n",
       "13               0.479072     0.827683     0.048307           0.580061  \n",
       "14                0.47739     0.850011     0.122411           0.555596  \n",
       "15               0.485031     0.849341     0.091742           0.547643  \n",
       "16               0.482302     0.848011     0.056527           0.546994  \n",
       "17                0.60741     0.754145     0.612712           0.589625  \n",
       "18               0.363304     0.000075     0.001224           0.932414  \n",
       "19               0.505785          0.0          0.0           0.845093  \n",
       "20               0.395119          0.0          0.0           0.866433  \n",
       "21               0.380665          0.0          0.0           0.864236  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "csv_dict['llama2-7b']['forget10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path('./leaderboard').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for model in model_family:\n",
    "    for rate in forget_rates:\n",
    "        model_name = 'llama' if model == 'llama2-7b' else 'phi'\n",
    "        rate_name = '1p' if rate == 'forget01' else '5p' if rate == 'forget05' else '10p'\n",
    "        save_dir = Path(f'./leaderboard/{model_name}-{rate_name}')\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        csv_dict[model][rate].to_csv(f'{save_dir}/{model_name}-{rate_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unlearning Method</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>LR</th>\n",
       "      <th>WD</th>\n",
       "      <th>Forget Quality</th>\n",
       "      <th>Model Utility</th>\n",
       "      <th>Prob. Retain</th>\n",
       "      <th>ROUGE Retain</th>\n",
       "      <th>Truth Ratio Retain</th>\n",
       "      <th>Prob. Real Authors</th>\n",
       "      <th>ROUGE Real Authors</th>\n",
       "      <th>Truth Ratio Real Authors</th>\n",
       "      <th>Prob. Real World</th>\n",
       "      <th>ROUGE Real World</th>\n",
       "      <th>Truth Ratio Real World</th>\n",
       "      <th>Prob. Forget</th>\n",
       "      <th>ROUGE Forget</th>\n",
       "      <th>Truth Ratio Forget</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finetune Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0012708143485281624</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.990939</td>\n",
       "      <td>0.98545</td>\n",
       "      <td>0.515985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retain Model</td>\n",
       "      <td>-1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.620744</td>\n",
       "      <td>0.990596</td>\n",
       "      <td>0.988945</td>\n",
       "      <td>0.464641</td>\n",
       "      <td>0.459313</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.603727</td>\n",
       "      <td>0.41217</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.538977</td>\n",
       "      <td>0.179521</td>\n",
       "      <td>0.397237</td>\n",
       "      <td>0.684941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.992639</td>\n",
       "      <td>0.94326</td>\n",
       "      <td>0.537666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>2</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.6206</td>\n",
       "      <td>0.988565</td>\n",
       "      <td>0.981648</td>\n",
       "      <td>0.475066</td>\n",
       "      <td>0.452178</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.592565</td>\n",
       "      <td>0.417651</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.538557</td>\n",
       "      <td>0.910844</td>\n",
       "      <td>0.795152</td>\n",
       "      <td>0.537782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>3</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.618734</td>\n",
       "      <td>0.988213</td>\n",
       "      <td>0.979334</td>\n",
       "      <td>0.475905</td>\n",
       "      <td>0.448936</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.590214</td>\n",
       "      <td>0.416042</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.53751</td>\n",
       "      <td>0.812619</td>\n",
       "      <td>0.685197</td>\n",
       "      <td>0.540144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>4</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.609718</td>\n",
       "      <td>0.976377</td>\n",
       "      <td>0.925874</td>\n",
       "      <td>0.476145</td>\n",
       "      <td>0.437524</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.57789</td>\n",
       "      <td>0.410665</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.537241</td>\n",
       "      <td>0.563045</td>\n",
       "      <td>0.552223</td>\n",
       "      <td>0.556348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grad. Ascent</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.606098</td>\n",
       "      <td>0.96861</td>\n",
       "      <td>0.899046</td>\n",
       "      <td>0.475981</td>\n",
       "      <td>0.433108</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.571714</td>\n",
       "      <td>0.408674</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.538149</td>\n",
       "      <td>0.45992</td>\n",
       "      <td>0.490859</td>\n",
       "      <td>0.562339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.992639</td>\n",
       "      <td>0.94326</td>\n",
       "      <td>0.537666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>2</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.620915</td>\n",
       "      <td>0.989375</td>\n",
       "      <td>0.980937</td>\n",
       "      <td>0.475476</td>\n",
       "      <td>0.452657</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.59322</td>\n",
       "      <td>0.417164</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539731</td>\n",
       "      <td>0.928414</td>\n",
       "      <td>0.812038</td>\n",
       "      <td>0.536927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>3</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.617538</td>\n",
       "      <td>0.988654</td>\n",
       "      <td>0.973717</td>\n",
       "      <td>0.475991</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.587574</td>\n",
       "      <td>0.414567</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.539067</td>\n",
       "      <td>0.830959</td>\n",
       "      <td>0.712933</td>\n",
       "      <td>0.536827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>4</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.608851</td>\n",
       "      <td>0.976676</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>0.477116</td>\n",
       "      <td>0.436046</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.573398</td>\n",
       "      <td>0.409009</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.539985</td>\n",
       "      <td>0.593071</td>\n",
       "      <td>0.562729</td>\n",
       "      <td>0.548445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Grad. Diff.</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.605267</td>\n",
       "      <td>0.969785</td>\n",
       "      <td>0.905299</td>\n",
       "      <td>0.47705</td>\n",
       "      <td>0.42991</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.567801</td>\n",
       "      <td>0.407527</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.541446</td>\n",
       "      <td>0.478168</td>\n",
       "      <td>0.503302</td>\n",
       "      <td>0.554357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.992639</td>\n",
       "      <td>0.94326</td>\n",
       "      <td>0.537666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>2</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.621588</td>\n",
       "      <td>0.988321</td>\n",
       "      <td>0.979643</td>\n",
       "      <td>0.468843</td>\n",
       "      <td>0.45515</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.593226</td>\n",
       "      <td>0.422742</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.53367</td>\n",
       "      <td>0.990477</td>\n",
       "      <td>0.965104</td>\n",
       "      <td>0.549335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>3</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.621138</td>\n",
       "      <td>0.98507</td>\n",
       "      <td>0.968064</td>\n",
       "      <td>0.462262</td>\n",
       "      <td>0.455434</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.594302</td>\n",
       "      <td>0.429384</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.533614</td>\n",
       "      <td>0.969096</td>\n",
       "      <td>0.818504</td>\n",
       "      <td>0.566585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>4</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.625006</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>0.938707</td>\n",
       "      <td>0.452894</td>\n",
       "      <td>0.46561</td>\n",
       "      <td>0.936333</td>\n",
       "      <td>0.609356</td>\n",
       "      <td>0.440029</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.544606</td>\n",
       "      <td>0.906534</td>\n",
       "      <td>0.638668</td>\n",
       "      <td>0.589092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pref. Opt.</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.623531</td>\n",
       "      <td>0.971541</td>\n",
       "      <td>0.922716</td>\n",
       "      <td>0.450949</td>\n",
       "      <td>0.464494</td>\n",
       "      <td>0.941333</td>\n",
       "      <td>0.605755</td>\n",
       "      <td>0.4413</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>0.544859</td>\n",
       "      <td>0.883273</td>\n",
       "      <td>0.42299</td>\n",
       "      <td>0.595564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KL Min.</td>\n",
       "      <td>1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.622677</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.985655</td>\n",
       "      <td>0.474699</td>\n",
       "      <td>0.455482</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>0.418562</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.539033</td>\n",
       "      <td>0.992639</td>\n",
       "      <td>0.94326</td>\n",
       "      <td>0.537666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KL Min.</td>\n",
       "      <td>2</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.620829</td>\n",
       "      <td>0.988561</td>\n",
       "      <td>0.98425</td>\n",
       "      <td>0.475031</td>\n",
       "      <td>0.45226</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.593925</td>\n",
       "      <td>0.417083</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.538285</td>\n",
       "      <td>0.91083</td>\n",
       "      <td>0.79128</td>\n",
       "      <td>0.537469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>KL Min.</td>\n",
       "      <td>3</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.619151</td>\n",
       "      <td>0.98817</td>\n",
       "      <td>0.976753</td>\n",
       "      <td>0.475435</td>\n",
       "      <td>0.449163</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.5895</td>\n",
       "      <td>0.416385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.538001</td>\n",
       "      <td>0.81142</td>\n",
       "      <td>0.68841</td>\n",
       "      <td>0.541105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>KL Min.</td>\n",
       "      <td>4</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.61018</td>\n",
       "      <td>0.976463</td>\n",
       "      <td>0.927744</td>\n",
       "      <td>0.475876</td>\n",
       "      <td>0.438618</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.577432</td>\n",
       "      <td>0.410813</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.536874</td>\n",
       "      <td>0.563253</td>\n",
       "      <td>0.534115</td>\n",
       "      <td>0.556943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>KL Min.</td>\n",
       "      <td>5</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.606016</td>\n",
       "      <td>0.968844</td>\n",
       "      <td>0.897789</td>\n",
       "      <td>0.476068</td>\n",
       "      <td>0.432337</td>\n",
       "      <td>0.9055</td>\n",
       "      <td>0.570292</td>\n",
       "      <td>0.408462</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.537498</td>\n",
       "      <td>0.460023</td>\n",
       "      <td>0.504543</td>\n",
       "      <td>0.562018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unlearning Method  Epoch     LR    WD         Forget Quality Model Utility  \\\n",
       "0     Finetune Model     -1  1e-05  0.01  0.0012708143485281624      0.622677   \n",
       "1       Retain Model     -1  1e-05  0.01                      1      0.620744   \n",
       "2       Grad. Ascent      1  1e-05  0.01               0.003018      0.622677   \n",
       "3       Grad. Ascent      2  1e-05  0.01               0.001271        0.6206   \n",
       "4       Grad. Ascent      3  1e-05  0.01               0.001271      0.618734   \n",
       "5       Grad. Ascent      4  1e-05  0.01               0.006761      0.609718   \n",
       "6       Grad. Ascent      5  1e-05  0.01               0.014302      0.606098   \n",
       "7        Grad. Diff.      1  1e-05  0.01               0.003018      0.622677   \n",
       "8        Grad. Diff.      2  1e-05  0.01               0.001271      0.620915   \n",
       "9        Grad. Diff.      3  1e-05  0.01               0.003018      0.617538   \n",
       "10       Grad. Diff.      4  1e-05  0.01               0.006761      0.608851   \n",
       "11       Grad. Diff.      5  1e-05  0.01               0.006761      0.605267   \n",
       "12        Pref. Opt.      1  1e-05  0.01               0.003018      0.622677   \n",
       "13        Pref. Opt.      2  1e-05  0.01               0.003018      0.621588   \n",
       "14        Pref. Opt.      3  1e-05  0.01               0.001271      0.621138   \n",
       "15        Pref. Opt.      4  1e-05  0.01               0.006761      0.625006   \n",
       "16        Pref. Opt.      5  1e-05  0.01               0.014302      0.623531   \n",
       "17           KL Min.      1  1e-05  0.01               0.003018      0.622677   \n",
       "18           KL Min.      2  1e-05  0.01               0.001271      0.620829   \n",
       "19           KL Min.      3  1e-05  0.01               0.001271      0.619151   \n",
       "20           KL Min.      4  1e-05  0.01               0.006761       0.61018   \n",
       "21           KL Min.      5  1e-05  0.01               0.014302      0.606016   \n",
       "\n",
       "   Prob. Retain ROUGE Retain Truth Ratio Retain Prob. Real Authors  \\\n",
       "0      0.989527     0.985655           0.474699           0.455482   \n",
       "1      0.990596     0.988945           0.464641           0.459313   \n",
       "2      0.989527     0.985655           0.474699           0.455482   \n",
       "3      0.988565     0.981648           0.475066           0.452178   \n",
       "4      0.988213     0.979334           0.475905           0.448936   \n",
       "5      0.976377     0.925874           0.476145           0.437524   \n",
       "6       0.96861     0.899046           0.475981           0.433108   \n",
       "7      0.989527     0.985655           0.474699           0.455482   \n",
       "8      0.989375     0.980937           0.475476           0.452657   \n",
       "9      0.988654     0.973717           0.475991           0.447673   \n",
       "10     0.976676     0.927235           0.477116           0.436046   \n",
       "11     0.969785     0.905299            0.47705            0.42991   \n",
       "12     0.989527     0.985655           0.474699           0.455482   \n",
       "13     0.988321     0.979643           0.468843            0.45515   \n",
       "14      0.98507     0.968064           0.462262           0.455434   \n",
       "15      0.97556     0.938707           0.452894            0.46561   \n",
       "16     0.971541     0.922716           0.450949           0.464494   \n",
       "17     0.989527     0.985655           0.474699           0.455482   \n",
       "18     0.988561      0.98425           0.475031            0.45226   \n",
       "19      0.98817     0.976753           0.475435           0.449163   \n",
       "20     0.976463     0.927744           0.475876           0.438618   \n",
       "21     0.968844     0.897789           0.476068           0.432337   \n",
       "\n",
       "   ROUGE Real Authors Truth Ratio Real Authors Prob. Real World  \\\n",
       "0               0.933                 0.596229         0.418562   \n",
       "1               0.928                 0.603727          0.41217   \n",
       "2               0.933                 0.596229         0.418562   \n",
       "3               0.923                 0.592565         0.417651   \n",
       "4               0.913                 0.590214         0.416042   \n",
       "5               0.898                  0.57789         0.410665   \n",
       "6              0.9055                 0.571714         0.408674   \n",
       "7               0.933                 0.596229         0.418562   \n",
       "8               0.923                  0.59322         0.417164   \n",
       "9               0.908                 0.587574         0.414567   \n",
       "10              0.898                 0.573398         0.409009   \n",
       "11              0.898                 0.567801         0.407527   \n",
       "12              0.933                 0.596229         0.418562   \n",
       "13              0.933                 0.593226         0.422742   \n",
       "14              0.933                 0.594302         0.429384   \n",
       "15           0.936333                 0.609356         0.440029   \n",
       "16           0.941333                 0.605755           0.4413   \n",
       "17              0.933                 0.596229         0.418562   \n",
       "18              0.923                 0.593925         0.417083   \n",
       "19              0.923                   0.5895         0.416385   \n",
       "20              0.903                 0.577432         0.410813   \n",
       "21             0.9055                 0.570292         0.408462   \n",
       "\n",
       "   ROUGE Real World Truth Ratio Real World Prob. Forget ROUGE Forget  \\\n",
       "0          0.882479               0.539033     0.990939      0.98545   \n",
       "1          0.882479               0.538977     0.179521     0.397237   \n",
       "2          0.882479               0.539033     0.992639      0.94326   \n",
       "3          0.882479               0.538557     0.910844     0.795152   \n",
       "4          0.884615                0.53751     0.812619     0.685197   \n",
       "5          0.884615               0.537241     0.563045     0.552223   \n",
       "6          0.880342               0.538149      0.45992     0.490859   \n",
       "7          0.882479               0.539033     0.992639      0.94326   \n",
       "8          0.882479               0.539731     0.928414     0.812038   \n",
       "9          0.884615               0.539067     0.830959     0.712933   \n",
       "10         0.880342               0.539985     0.593071     0.562729   \n",
       "11         0.880342               0.541446     0.478168     0.503302   \n",
       "12         0.882479               0.539033     0.992639      0.94326   \n",
       "13         0.893162                0.53367     0.990477     0.965104   \n",
       "14         0.888889               0.533614     0.969096     0.818504   \n",
       "15         0.880342               0.544606     0.906534     0.638668   \n",
       "16         0.880342               0.544859     0.883273      0.42299   \n",
       "17         0.882479               0.539033     0.992639      0.94326   \n",
       "18         0.884615               0.538285      0.91083      0.79128   \n",
       "19         0.884615               0.538001      0.81142      0.68841   \n",
       "20         0.884615               0.536874     0.563253     0.534115   \n",
       "21         0.888889               0.537498     0.460023     0.504543   \n",
       "\n",
       "   Truth Ratio Forget  \n",
       "0            0.515985  \n",
       "1            0.684941  \n",
       "2            0.537666  \n",
       "3            0.537782  \n",
       "4            0.540144  \n",
       "5            0.556348  \n",
       "6            0.562339  \n",
       "7            0.537666  \n",
       "8            0.536927  \n",
       "9            0.536827  \n",
       "10           0.548445  \n",
       "11           0.554357  \n",
       "12           0.537666  \n",
       "13           0.549335  \n",
       "14           0.566585  \n",
       "15           0.589092  \n",
       "16           0.595564  \n",
       "17           0.537666  \n",
       "18           0.537469  \n",
       "19           0.541105  \n",
       "20           0.556943  \n",
       "21           0.562018  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict['llama2-7b']['forget01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
